# -*- coding: utf-8 -*-
"""
Spyder Editor

This is a temporary script file.
"""
import numpy as np
#print(np.__version__)
#import scipy.ndimage
import tensorflow as tf # Requires numpy 1.26.3

import PIL
from PIL import Image
import os
from PIL import ImageChops
import tensorflow.keras.layers as layers
from tensorflow.keras.applications import EfficientNetB3,ResNet50
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from collections import Counter
from sklearn.metrics import classification_report, accuracy_score
import cv2
import glob
import keras
from keras import layers


os.environ['KMP_WARNINGS'] = '0'


dataset_path = "/Users/chenpinyu/Desktop/advanced_analytics/assign2/noBorderimages"

physical_devices = tf.config.list_physical_devices('GPU')
if physical_devices:
    tf.config.set_logical_device_configuration(
        physical_devices[0], [tf.config.LogicalDeviceConfiguration(memory_limit=4096)])

'''
def remove_black_border(image_path, save_path, threshold=10, target_size=(256, 256)):
    """ å»é™¤é»‘è‰²é‚Šæ¡†ä¸¦èª¿æ•´å¤§å° """
    try:
        image = Image.open(image_path).convert("RGB")
        image_array = np.array(image)
        
        # å»ºç«‹éé»‘è‰²å€åŸŸçš„é®ç½©
        mask = np.any(image_array > [threshold, threshold, threshold], axis=-1)
        
        # æ‰¾å‡ºéé»‘è‰²å€åŸŸçš„é‚Šç•Œ
        coords = np.argwhere(mask)
        if coords.size == 0:
            print(f"âš ï¸ å…¨é»‘åœ–ç‰‡ï¼Œç„¡æ³•è£å‰ª: {image_path}")
            return
        
        y_min, x_min = coords.min(axis=0)
        y_max, x_max = coords.max(axis=0)

        # ç¢ºä¿ä¸æœƒç™¼ç”Ÿåè½‰
        if x_max > x_min and y_max > y_min:
            cropped_image = image.crop((x_min, y_min, x_max + 1, y_max + 1))
        else:
            cropped_image = image  # å¦‚æœæ²’æ³•è£å‰ªï¼Œå‰‡ä½¿ç”¨åŸåœ–

        # ç¸®æ”¾è‡³æŒ‡å®šå¤§å°
        resized_image = cropped_image.resize((512, 256), Image.LANCZOS)

        # ç¢ºä¿å„²å­˜è·¯å¾‘å­˜åœ¨
        os.makedirs(os.path.dirname(save_path), exist_ok=True)

        resized_image.save(save_path)
        print(f"âœ… è™•ç†å®Œæˆ: {image_path} -> {save_path}")
    except Exception as e:
        print(f"âŒ éŒ¯èª¤ç™¼ç”Ÿ: {image_path}: {e}")

def process_folder(input_folder, output_folder):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    
    total_images = 0
    processed_images = 0
    
    for root, _, files in os.walk(input_folder):
        relative_path = os.path.relpath(root, input_folder)
        save_dir = os.path.join(output_folder, relative_path)
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        
        for file in files:
            if file.lower().endswith(('png', 'jpg', 'jpeg')):
                total_images += 1
                image_path = os.path.join(root, file)
                save_path = os.path.join(save_dir, file)  # Save in separate folder
                if os.path.exists(image_path):
                    remove_black_border(image_path, save_path)
                    processed_images += 1
    
    print(f"Total images found: {total_images}")
    print(f"Total images processed: {processed_images}")
# Change these paths accordingly
main_folder = "/Users/chenpinyu/Desktop/assign2/images"
output_folder = "/Users/chenpinyu/Desktop/assign2/noBorderimages"
process_folder(main_folder, output_folder)
print("Processing complete!")


def convert_jpg_to_png(image_path):
    """ å°† JPG/JPEG è½¬æ¢ä¸º PNG """
    try:
        img = cv2.imread(image_path)
        if img is None:
            print(f"âŒ æ— æ³•è¯»å–å›¾åƒ: {image_path}")
            return
        
        new_path = image_path.rsplit('.', 1)[0] + ".png"  # æ›¿æ¢åç¼€
        cv2.imwrite(new_path, img)  # ä¿å­˜ä¸º PNG
        print(f"âœ… è½¬æ¢æˆåŠŸ: {new_path}")

    except Exception as e:
        print(f"âš ï¸ è½¬æ¢å¤±è´¥ {image_path}:  {e}")

# éå†æ‰€æœ‰å›½å®¶çš„æ–‡ä»¶å¤¹
for class_name in os.listdir(dataset_path):
    class_path = os.path.join(dataset_path, class_name)
    
    if not os.path.isdir(class_path):  
        continue  # è·³è¿‡éæ–‡ä»¶å¤¹é¡¹

    # éå†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰å›¾åƒæ–‡ä»¶
    for file_name in os.listdir(class_path):
        if file_name.lower().endswith(('.jpg', '.jpeg')):  # åªå¤„ç† JPG/JPEG æ–‡ä»¶
            file_path = os.path.join(class_path, file_name)
            convert_jpg_to_png(file_path)  # æ‰§è¡Œè½¬æ¢

for class_name in os.listdir(dataset_path):
    class_path = os.path.join(dataset_path, class_name)
    if os.path.isdir(class_path):  
        for jpg_file in glob.glob(os.path.join(class_path, "*.jpg")):
            os.remove(jpg_file)
        for jpeg_file in glob.glob(os.path.join(class_path, "*.jpeg")):
            os.remove(jpeg_file)
print("ğŸ—‘ï¸ å·²åˆ é™¤æ‰€æœ‰ JPG/JPEG æ–‡ä»¶ï¼Œåªå‰© PNG")

'''
# Set image size and batch size
#IMG_SIZE = 224
BATCH_SIZE = 16
NUM_CLASSES = 12 

# Augmentation and Preprocessing
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    brightness_range=[0.8, 1.2], 
    validation_split=0.2  # 80/20 train-validation split
)

#Image.MAX_IMAGE_PIXELS = None  # å–æ¶ˆ PIL åœ–ç‰‡å¤§å°é™åˆ¶

# Load training data
train_generator = train_datagen.flow_from_directory(
    dataset_path ,
    target_size=(512, 256),
    batch_size=BATCH_SIZE,
    class_mode="sparse",  # Use "sparse" for integer labels
    subset="training"
)

# Load validation data
val_generator = train_datagen.flow_from_directory(
    dataset_path ,
    target_size=(512, 256),
    batch_size=BATCH_SIZE,
    class_mode="sparse",
    subset="validation"
)



# Load pre-trained EfficientNetB3 (without the top classification layer)
#base_model = EfficientNetB3(weights="imagenet", include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))

# Freeze base model layers (fine-tuning later)
#base_model.trainable = False



# pretrained mmodel: ResNet50
base_model = ResNet50(weights="imagenet", include_top=False, input_shape=(512, 256, 3))
base_model.trainable = False  # åˆå§‹æ™‚å‡çµé è¨“ç·´å±¤

# Custom classifier
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(512, activation="relu")(x)
x = layers.Dropout(0.3)(x)  # Prevent overfitting
x = layers.Dense(NUM_CLASSES, activation="softmax")(x)  # 12 country classes

# Define full model
model = Model(inputs=base_model.input, outputs=x)

# å»ºç«‹ CustomModel
#model = CustomModel(inputs=base_model.input, outputs=x)

# Compile model EfficientNet

model.compile(optimizer=Adam(learning_rate=0.0001),
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])

# Model Summary
#model.summary()

# Train model
history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=30,  # Adjust based on training time
    verbose=1
)

# **è§£å‡éƒ¨åˆ†ResNetå±¤é€²è¡ŒFine-Tuning**
base_model.trainable = True
for layer in base_model.layers[:100]:  # åªè§£å‡æœ€å¾Œ50å±¤
    layer.trainable = False

# é‡æ–°ç·¨è­¯
model.compile(optimizer=Adam(learning_rate=0.00001),  # èª¿ä½å­¸ç¿’ç‡
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])

# å†æ¬¡è¨“ç·´
history_fine = model.fit(
    train_generator,
    epochs=10,
    validation_data=val_generator,
    verbose=1
)

# Evaluate on validation set
val_generator.reset()
y_true = val_generator.classes
y_pred = model.predict(val_generator)
y_pred_classes = np.argmax(y_pred, axis=1)

# Print evaluation metrics
print("Classification Report:")
print(classification_report(y_true, y_pred_classes, target_names=list(train_generator.class_indices.keys())))
print(f"Validation Accuracy: {accuracy_score(y_true, y_pred_classes):.4f}")

# **å¯è¦–åŒ–è¨“ç·´çµæœ**
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend()
plt.show()

# **å„²å­˜æ¨¡å‹**
model.save("/Users/chenpinyu/Desktop/advanced_analytics/geoguessr_country_model.keras")

